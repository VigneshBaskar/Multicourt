{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.2 |Anaconda 4.2.0 (64-bit)| (default, Jul  5 2016, 11:41:13) [MSC v.1900 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "#Use Python 3.5\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import gensim\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from inspect import getsourcefile\n",
    "from os.path import abspath,relpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers.wrappers import TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "current_path=abspath(getsourcefile(lambda:0)).replace(relpath(getsourcefile(lambda:0)),\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for x in tgAi_w2v_temp_model.most_similar(u\"elisabeth\"):\n",
    "#     print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pickle_open(file_name):\n",
    "    with open(file_name, 'rb') as f:\n",
    "        u = pickle._Unpickler(f)\n",
    "        u.encoding = 'latin1'\n",
    "        return u.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Word2Vector_models_path=os.path.join(current_path,\"Word2Vector_models\")\n",
    "fname=\"tgi_w2v_temp_model_python_3.w2v\"\n",
    "tgi_w2v_temp_model = Word2Vec.load(os.path.join(Word2Vector_models_path,fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Data comes from Training_set_labeling_clean.ipynb\n",
    "training_pickle_path=os.path.join(current_path,\"Training_Pickle\")\n",
    "training_data=pickle_open(os.path.join(training_pickle_path,\"tgi_training_data_pickle_2.p\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def label_data(string,sub_strings,sub_strings_label,w2v_model=tgi_w2v_temp_model,word2vec_length=150):\n",
    "    #String is a Unicode\n",
    "    #Function comes from Training_set_labeling_clean.ipynb\n",
    "    #Take the string and identify the boundaries where judge names lie\n",
    "    divisions=[0,len(string)]\n",
    "    for sub_string in sub_strings:\n",
    "        if (sub_string!=0) and (len(re.findall(sub_string,string))!=0):\n",
    "            for m in re.finditer(sub_string,string):\n",
    "                divisions.append(m.start())\n",
    "                divisions.append(m.end())\n",
    "    divisions=sorted(divisions)\n",
    "\n",
    "    div_string=[]\n",
    "    for index in range(len(divisions)-1):\n",
    "        div_string.append(string[divisions[index]:divisions[index+1]])\n",
    "\n",
    "    div_string_label=[]\n",
    "    for sub_string,sub_string_label in zip(sub_strings,sub_strings_label):\n",
    "        sub_string=str(sub_string)\n",
    "        #print(type(sub_string))\n",
    "        div_string_label.append(sub_string_label*np.array(np.asarray(div_string)==np.asarray(sub_string)))\n",
    "\n",
    "    div_string_label=np.sum(div_string_label,0)\n",
    "\n",
    "    tokenized_string=[]\n",
    "    tokenized_string_label=[]\n",
    "    for index in range(len(div_string)):\n",
    "        temp=word_tokenize(div_string[index])\n",
    "        tokenized_string+=temp\n",
    "        try:\n",
    "            tokenized_string_label+=[int(div_string_label[index])]*len(temp)\n",
    "        except:\n",
    "            tokenized_string_label+=[int(div_string_label)]*len(temp)\n",
    "    tokens_lower=[word.lower() for word in tokenized_string]\n",
    "    mul=2\n",
    "    \n",
    "    tokens_shape_features=[[mul*int(str(word).isnumeric()) if int(str(word).isnumeric()) else mul*-1,mul*int(str(word).isalpha()) if int(str(word).isalpha()) else mul*-1,mul*int(str(word).islower()) if int(str(word).islower()) else mul*-1,mul*int(str(word).isupper()) if int(str(word).isupper()) else mul*-1] for word in tokenized_string]\n",
    "    tokens_word2vec_features=[]\n",
    "    \n",
    "    for word in tokens_lower:\n",
    "        try:\n",
    "            tokens_word2vec_features.append(list(w2v_model[word]))\n",
    "        except:\n",
    "            tokens_word2vec_features.append(list(np.random.rand(word2vec_length,1)))\n",
    "    for index in range(len(tokenized_string)):\n",
    "        if tokenized_string[index]==\"0.0\":\n",
    "            tokenized_string_label[index]=0\n",
    "    return tokenized_string,tokenized_string_label,np.hstack((tokens_word2vec_features,tokens_shape_features)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def token_to_file_features(tokenized_string,tokenized_string_label,token_features,sent_len=20):\n",
    "    file_features=[]\n",
    "    for temp in [tokenized_string,tokenized_string_label,token_features]:\n",
    "        sent_boundaries=list(np.arange(0,len(temp),sent_len))+[len(temp)]\n",
    "        sent_features=[]\n",
    "        for x in range(len(sent_boundaries)-1):\n",
    "            sent_features.append(temp[sent_boundaries[x]:sent_boundaries[x+1]])\n",
    "        file_features.append(np.array(sent_features).tolist())\n",
    "        \n",
    "    return file_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_file_tokens=[]\n",
    "all_file_features=[]\n",
    "all_file_labels=[]\n",
    "\n",
    "for num in range(0,len(training_data)):\n",
    "    #print(str(num),end=\",\")\n",
    "    string=training_data.loc[num][\"text_data\"]\n",
    "    sub_strings=[training_data.loc[num][\"judge_name_1\"],training_data.loc[num][\"judge_name_2\"],training_data.loc[num][\"judge_name_3\"]]\n",
    "    sub_strings_label=[1,2,3]\n",
    "    \n",
    "    tokenized_string,tokenized_string_label,token_features=label_data(string,sub_strings,sub_strings_label)\n",
    "    single_tokenized_string,single_tokenized_string_label,single_token_features=token_to_file_features(tokenized_string,tokenized_string_label,token_features)\n",
    "    all_file_tokens.append(single_tokenized_string)\n",
    "    all_file_features.append(single_token_features)\n",
    "    all_file_labels.append(single_tokenized_string_label)\n",
    "    #print(\" \".join(np.hstack(single_tokenized_string)[np.hstack(single_tokenized_string_label)!=0])+\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train=[sequence.pad_sequences(all_file_features[i],maxlen=20,dtype='float32',padding='post',truncating='post') for i in range(100)]\n",
    "X_train_padded=np.vstack(X_train)\n",
    "\n",
    "X_train_tokens=[sequence.pad_sequences(all_file_tokens[i],maxlen=20,dtype='str',padding='post',truncating='post') for i in range(100)]\n",
    "X_train_tokens_padded=np.vstack(X_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_file_labels_array=np.asarray(all_file_labels)\n",
    "for j in range(len((all_file_labels_array))):\n",
    "    temp=[]\n",
    "    for i in range(len(all_file_labels_array[j])):\n",
    "        temp.append(to_categorical(all_file_labels_array[j][i],len(np.unique(sub_strings_label))+1))\n",
    "    all_file_labels_array[j]=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train=[sequence.pad_sequences(all_file_labels_array[i],maxlen=20,dtype='float32',padding='post',truncating='post') for i in range(100)]\n",
    "y_train_padded=np.vstack(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(520, 20) (520, 20, 4) (520, 20, 154)\n"
     ]
    }
   ],
   "source": [
    "temp=X_train_tokens_padded[y_train_padded[:,:,0]!=1]\n",
    "\n",
    "temp1=X_train_tokens_padded[temp!=\"0.0\"]\n",
    "temp2=y_train_padded[temp!=\"0.0\"]\n",
    "temp3=X_train_padded[temp!=\"0.0\"]\n",
    "\n",
    "print(temp1.shape,temp2.shape,temp3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rep=50\n",
    "temp=[temp1 for n in range(rep)]\n",
    "X_train_tokens_padded_new=np.vstack((X_train_tokens_padded,np.array(np.vstack(temp))))\n",
    "\n",
    "temp=[temp2 for n in range(rep)]\n",
    "y_train_padded_new=np.vstack((y_train_padded,np.array(np.vstack(temp))))\n",
    "\n",
    "temp=[temp3 for n in range(rep)]\n",
    "X_train_padded_new=np.vstack((X_train_padded,np.array(np.vstack(temp))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers.wrappers import Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "\n",
    "model.add(Bidirectional(LSTM(output_dim=32,return_sequences=True,activation='sigmoid',inner_activation='hard_sigmoid'),input_shape=(20, 154)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(8))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(TimeDistributed(Dense(len(np.unique(sub_strings_label))+1)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adagrad',metrics=['accuracy','fmeasure'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "38655/38655 [==============================] - 544s - loss: 0.0082 - acc: 0.9981 - fmeasure: 0.9970   \n",
      "Epoch 2/20\n",
      "38655/38655 [==============================] - 538s - loss: 0.0076 - acc: 0.9981 - fmeasure: 0.9970   \n",
      "Epoch 3/20\n",
      "38655/38655 [==============================] - 537s - loss: 0.0071 - acc: 0.9981 - fmeasure: 0.9971   \n",
      "Epoch 4/20\n",
      "38655/38655 [==============================] - 539s - loss: 0.0067 - acc: 0.9981 - fmeasure: 0.9971   \n",
      "Epoch 5/20\n",
      "38655/38655 [==============================] - 663s - loss: 0.0064 - acc: 0.9982 - fmeasure: 0.9971   \n",
      "Epoch 6/20\n",
      "38655/38655 [==============================] - 659s - loss: 0.0061 - acc: 0.9983 - fmeasure: 0.9972   \n",
      "Epoch 7/20\n",
      "38655/38655 [==============================] - 673s - loss: 0.0058 - acc: 0.9982 - fmeasure: 0.9972   \n",
      "Epoch 8/20\n",
      "38655/38655 [==============================] - 653s - loss: 0.0057 - acc: 0.9983 - fmeasure: 0.9973   \n",
      "Epoch 9/20\n",
      "38655/38655 [==============================] - 614s - loss: 0.0055 - acc: 0.9983 - fmeasure: 0.9973   \n",
      "Epoch 10/20\n",
      "38655/38655 [==============================] - 928s - loss: 0.0056 - acc: 0.9983 - fmeasure: 0.9972   \n",
      "Epoch 11/20\n",
      "38655/38655 [==============================] - 1012s - loss: 0.0054 - acc: 0.9983 - fmeasure: 0.9972  \n",
      "Epoch 12/20\n",
      "38655/38655 [==============================] - 972s - loss: 0.0052 - acc: 0.9983 - fmeasure: 0.9973   \n",
      "Epoch 13/20\n",
      "38655/38655 [==============================] - 1026s - loss: 0.0052 - acc: 0.9983 - fmeasure: 0.9973  \n",
      "Epoch 14/20\n",
      "38655/38655 [==============================] - 971s - loss: 0.0052 - acc: 0.9983 - fmeasure: 0.9973   \n",
      "Epoch 15/20\n",
      "10010/38655 [======>.......................] - ETA: 701s - loss: 0.0050 - acc: 0.9984 - fmeasure: 0.9974"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_padded_new, y_train_padded_new, batch_size=1, nb_epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     X_valid=[sequence.pad_sequences(all_file_features[i],maxlen=20,dtype='float32',padding='post',truncating='post') ]\n",
    "#     X_valid_padded=np.vstack(X_valid)\n",
    "\n",
    "#     X_valid_tokens=[sequence.pad_sequences(all_file_tokens[i],maxlen=20,dtype='str',padding='post',truncating='post')]\n",
    "#     X_valid_tokens_padded=np.vstack(X_valid_tokens)\n",
    "\n",
    "#     y_valid=[sequence.pad_sequences(all_file_labels_array[i],maxlen=20,dtype='float32',padding='post',truncating='post')]\n",
    "#     y_valid_padded=np.vstack(y_valid)\n",
    "\n",
    "#     valid_prediction=model.predict(X_valid_padded,batch_size=100)\n",
    "#     valid_prediction_classes=model.predict_classes(X_valid_padded,batch_size=100)\n",
    "\n",
    "#     print(X_valid_tokens_padded[valid_prediction_classes!=0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "actual=X_valid_tokens_padded[y_valid_padded[:,:,0]!=1][X_valid_tokens_padded[y_valid_padded[:,:,0]!=1]!=\"0.0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_valid=[sequence.pad_sequences(all_file_features[i],maxlen=20,dtype='float32',padding='post',truncating='post') for i in range(100,len(training_data))]\n",
    "X_valid_padded=np.vstack(X_valid)\n",
    "\n",
    "X_valid_tokens=[sequence.pad_sequences(all_file_tokens[i],maxlen=20,dtype='str',padding='post',truncating='post') for i in range(100,len(training_data))]\n",
    "X_valid_tokens_padded=np.vstack(X_valid_tokens)\n",
    "\n",
    "y_valid=[sequence.pad_sequences(all_file_labels_array[i],maxlen=20,dtype='float32',padding='post',truncating='post') for i in range(100,len(training_data))]\n",
    "y_valid_padded=np.vstack(y_valid)\n",
    "\n",
    "valid_prediction=model.predict(X_valid_padded,batch_size=100)\n",
    "valid_prediction_classes=model.predict_classes(X_valid_padded,batch_size=100)\n",
    "\n",
    "result=X_valid_tokens_padded[valid_prediction_classes!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actual_copy=actual.copy()\n",
    "result_copy=result.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "actual_list=list(actual)\n",
    "actual_copy_list=list(actual_copy)\n",
    "result_list=list(result)\n",
    "result_copy_list=list(result_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for item in result_list:\n",
    "    if item in actual_list:\n",
    "        actual_list.remove(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recall=1-(len(actual_list)/len(actual_copy_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for item in actual_copy_list:\n",
    "    if item in result_list:\n",
    "        result_list.remove(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "precision=1-(len(result_list)/len(result_copy_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(precision,recall)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
